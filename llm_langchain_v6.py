from curl_cffi.requests import Session
import yfinance as yf
import pandas as pd
import numpy as np
import ffn
from langchain_core.tools import tool
# from duckduckgo_search import DDGS
from openai import OpenAI
import os
import html
import urllib.parse
import requests
from bs4 import BeautifulSoup
from googlesearch import search  # google ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
import requests
from bs4 import BeautifulSoup
import os
from pytrends.request import TrendReq
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from no_ssl_verification import no_ssl_verification
from transformers import pipeline
load_dotenv()
import time
from collections import Counter
import praw
from openai import OpenAI
from dotenv import load_dotenv
import os
from no_ssl_verification import no_ssl_verification
# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

import pandas as pd
import os
import numpy as np
import openai
import json 
import requests
from requests.auth import HTTPBasicAuth
from bs4 import BeautifulSoup

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')

# Reddit ì¸ì¦ ì •ë³´
reddit_client_id = "RVKUBtrh7ExzRSbddfBDtg"
reddit_client_secret = "cT4m_YrJnZhZpZ2vPkVTfMT8hqj07A"
reddit_user_agent = "retail_stock_v1.0 (by /u/TraditionalIce9098)"

# Reddit í´ë¼ì´ì–¸íŠ¸ ìƒì„±
reddit = praw.Reddit(
    client_id=reddit_client_id,
    client_secret=reddit_client_secret,
    user_agent=reddit_user_agent,
    check_for_async=False
)

# GPTë¥¼ ì‚¬ìš©í•´ ê´€ë ¨ ì¢…ëª© ì¶”ì¶œ
def extract_related_tickers(title, body, model="gpt-4o"):
    prompt = f"""
ë‹¤ìŒ Reddit ê²Œì‹œê¸€ ì œëª©, ë³¸ë¬¸, ì´ë¯¸ì§€ ë“±ì—ì„œ ì–¸ê¸‰ëœ ì£¼ìš” ì£¼ì‹ ì¢…ëª©(symbol)ì„ ìµœëŒ€ 3ê°œê¹Œì§€ ì¶”ì¶œí•˜ì„¸ìš”.
ì£¼ì‹ì€ í‹°ì»¤ë¡œ ì–¸ê¸‰ë  ìˆ˜ë„ ìˆì§€ë§Œ, ì¢…ëª©ëª…(ì˜ˆ: Microsoft;MSFT)ìœ¼ë¡œ ì–¸ê¸‰ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
ë¯¸êµ­ ì£¼ì‹ ê¸°ì¤€ìœ¼ë¡œ ì¢…ëª© ì½”ë“œ(TSLA, GME ë“±)ë¥¼ ë°˜í™˜í•˜ê³ , ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.

ì œëª©: {title}
ë³¸ë¬¸: {body[:1000]}

ê²°ê³¼ëŠ” JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì£¼ì„¸ìš”. ì˜ˆ: ["TSLA", "AAPL"]
"""
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful financial assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
        )
        content = response.choices[0].message.content.strip()
        tickers = eval(content) if content.startswith("[") else []
        return tickers
    except Exception as e:
        print(f"âŒ GPT ì˜¤ë¥˜: {e}")
        return []







with no_ssl_verification():
    pipe = pipeline("text-classification", model="snunlp/KR-FinBert-SC")

llm = ChatOpenAI(model = 'gpt-4o')



openai_api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=openai_api_key)

session = Session(
        impersonate="chrome110",  # optional, mimics Chrome browser
        headers={
            "User-Agent": "Mozilla/5.0"
        },
        verify=False  # optional; disable SSL verify if you're behind a proxy
    )
def get_yf_close_prices(tickers: list[str], period: str) -> pd.DataFrame:
    ticker_objs = yf.Tickers(" ".join(tickers),  session= session)
    history_dict = {
        symbol: ticker_objs.tickers[symbol].history(period=period)
        for symbol in tickers
    }

    # ê° í‹°ì»¤ì˜ 'Close' ì‹œë¦¬ì¦ˆë¥¼ ëª¨ì•„ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ
    close_df = pd.DataFrame({
        symbol: df["Close"]
        for symbol, df in history_dict.items()
        if "Close" in df.columns
    })
    close_df.index = close_df.index.strftime("%Y-%m-%d")
    close_df.index = pd.DatetimeIndex(close_df.index)
    return close_df

def get_returns_df(df, N=1, log=False):
    if log:
        return np.log(df / df.shift(N)).iloc[N-1:].fillna(0)
    else:
        return df.pct_change(N, fill_method=None).iloc[N-1:].fillna(0)

def get_cum_returns_df(return_df, log=False):
    if log:
        return np.exp(return_df.cumsum())
    else:
        return (1 + return_df).cumprod()    # same with (return_df.cumsum() + 1)

def get_CAGR_series(cum_rtn_df, num_day_in_year=250):
    cagr_series = cum_rtn_df.iloc[-1]**(num_day_in_year/(len(cum_rtn_df))) - 1
    return cagr_series

def get_sharpe_ratio(log_rtn_df, yearly_rfr = 0.025):
    excess_rtns = log_rtn_df.mean()*252 - yearly_rfr
    return excess_rtns / (log_rtn_df.std() * np.sqrt(252))

def get_drawdown_infos(cum_returns_df): 
    # 1. Drawdown
    cummax_df = cum_returns_df.cummax()
    dd_df = cum_returns_df / cummax_df - 1
 
    # 2. Maximum drawdown
    mdd_series = dd_df.min()

    # 3. longest_dd_period
    dd_duration_info_list = list()
    max_point_df = dd_df[dd_df == 0]
    for col in max_point_df:
        _df = max_point_df[col]
        _df.loc[dd_df[col].last_valid_index()] = 0
        _df = _df.dropna()

        periods = _df.index[1:] - _df.index[:-1]

        days = periods.days
        max_idx = days.argmax()

        longest_dd_period = days.max()
        dd_mean = int(np.mean(days))
        dd_std = int(np.std(days))

        dd_duration_info_list.append(
            [
                dd_mean,
                dd_std,
                longest_dd_period,
                "{} ~ {}".format(_df.index[:-1][max_idx].date(), _df.index[1:][max_idx].date())
            ]
        )

    dd_duration_info_df = pd.DataFrame(
        dd_duration_info_list,
        index=dd_df.columns,
        columns=['drawdown mean', 'drawdown std', 'longest days', 'longest period']
    )
    return dd_df, mdd_series, dd_duration_info_df

def get_rebal_dates(price_df, period="month"):
    _price_df = price_df.reset_index()
    if period == "month":
         groupby = [_price_df['Date'].dt.year, _price_df['Date'].dt.month]
    elif period == "quarter":
        groupby = [_price_df['Date'].dt.year, _price_df['Date'].dt.quarter]
    elif period == "halfyear":
        groupby = [_price_df['Date'].dt.year, _price_df['Date'].dt.month // 7]
    elif period == "year":
        groupby = [_price_df['Date'].dt.year, _price_df['Date'].dt.year]
    rebal_dates = pd.to_datetime(_price_df.groupby(groupby)['Date'].last().values)
    return rebal_dates

from functools import reduce

def calculate_portvals(price_df, weight_df):
    cum_rtn_up_until_now = 1 
    individual_port_val_df_list = []

    prev_end_day = weight_df.index[0]
    for end_day in weight_df.index[1:]:
        sub_price_df = price_df.loc[prev_end_day:end_day]
        sub_asset_flow_df = sub_price_df / sub_price_df.iloc[0]

        weight_series = weight_df.loc[prev_end_day]
        indi_port_cum_rtn_series = (sub_asset_flow_df * weight_series) * cum_rtn_up_until_now
    
        individual_port_val_df_list.append(indi_port_cum_rtn_series)

        total_port_cum_rtn_series = indi_port_cum_rtn_series.sum(axis=1)
        cum_rtn_up_until_now = total_port_cum_rtn_series.iloc[-1]

        prev_end_day = end_day 

    individual_port_val_df = reduce(lambda x, y: pd.concat([x, y.iloc[1:]]), individual_port_val_df_list)
    return individual_port_val_df

def get_backtest(ticker_list, period:str = '5y'):
    # Redditì—ì„œ ticker get
    tickers = ticker_list
    df_close = get_yf_close_prices(tickers, period)
    index = ['SPY', 'QQQ']
    df_index = get_yf_close_prices(index, "5y")

    rebal_dates = get_rebal_dates(df_close, 'month')
    rebal_index = rebal_dates
    result_portval_dict = {} 
    n_assets = df_close.shape[1]
    target_ratios = np.array([1/n_assets] * n_assets)

    target_weight_df = pd.DataFrame(
        [[1/len(df_close.columns)]*len(df_close.columns)]* len(rebal_index),
        index=rebal_index,
        columns=df_close.columns
    )

    cum_rtn_at_last_month_end = 1
    individual_port_val_df_list = []

    individual_port_val_df = calculate_portvals(df_close, target_weight_df)
    individual_port_val_df.head()
    result_portval_dict['port'] = individual_port_val_df.sum(axis=1)

    import ffn
    pd.concat([pd.DataFrame(result_portval_dict), df_index], axis = 1).dropna().rebase().plot();
    df_all = pd.concat([pd.DataFrame(result_portval_dict), df_index], axis = 1).dropna().rebase()

        # êµ¬ê°„ë³„
    stats = df_all.rebase().calc_stats()
    stats.display()
    # stats.stats.to_clipboard()

    # qs.reports.full(df_all.loc[:,'port'], df_all.loc[:, 'SPY']).to_markdown() 
    return df_all.to_markdown()

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from graph_chart import draw_backtest_chart
from langchain_core.tools import tool
from datetime import datetime
import pytz
import yfinance as yf
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
import streamlit.components.v1 as components
from dotenv import load_dotenv
load_dotenv()


llm = ChatOpenAI(model = 'gpt-4o')

# ë„êµ¬ í•¨ìˆ˜ ì •ì˜
@tool
def get_current_time(timezone: str, location: str) -> str:
    """í˜„ì¬ ì‹œê°ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜."""
    try:
        tz = pytz.timezone(timezone)
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        result = f'{timezone} ({location}) í˜„ì¬ì‹œê° {now}'
        print(result)
        return result
    except pytz.UnknownTimeZoneError:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì„ì¡´: {timezone}"

@tool
def get_yf_stock_history(ticker: str, period: str) -> str:
    """
    ì¢…ëª©ì˜ ì£¼ê°€ ì´ë ¥ì„ ì¡°íšŒí•´ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì‹œê°í™”ëŠ” ë³„ë„ ë„êµ¬ì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    stock = yf.Ticker(ticker, session= session)
    df = stock.history(period=period)

    if df.empty:
        return f"{ticker}ì˜ {period} ê¸°ê°„ ì£¼ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    # âœ… ì‹œê°í™”ë¥¼ ìœ„í•´ ë°ì´í„° ì €ì¥
    st.session_state["latest_history_chart"] = df[["Close"]].copy()
    st.session_state["latest_history_chart"].index = st.session_state["latest_history_chart"].index.strftime("%Y-%m-%d")

    return df.tail().to_markdown()

@tool
def get_yf_stock_info(ticker: str) -> str:
    """í•´ë‹¹ ì¢…ëª©ì˜ Yahoo Finance ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    stock = yf.Ticker(ticker, session= session)
    info = stock.info
    return str(info)

@tool
def get_yf_stock_recommendations(ticker: str) -> str:
    """í•´ë‹¹ ì¢…ëª©ì˜ ë¦¬ì„œì¹˜ ì¶”ì²œ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    stock = yf.Ticker(ticker, session= session)
    recommendations = stock.recommendations
    if recommendations is None or recommendations.empty:
        return f"{ticker}ì— ëŒ€í•œ ì¶”ì²œ ë¦¬ì„œì¹˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    return recommendations.to_markdown()

@tool
def get_backtest_tool(ticker_list: list[str], period: str = "5y") -> str:
    """
    ì£¼ì–´ì§„ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ì™€ ê¸°ê°„ìœ¼ë¡œ ë°±í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³ ,
    ëˆ„ì  ìˆ˜ìµë¥  ì°¨íŠ¸ë¥¼ ì‹œê°í™”í•˜ë©°, ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        df_close = get_yf_close_prices(ticker_list, period)
        index = ['SPY', 'QQQ']
        df_index = get_yf_close_prices(index, "5y")

        rebal_dates = get_rebal_dates(df_close, 'month')
        rebal_index = rebal_dates

        n_assets = df_close.shape[1]
        target_weight_df = pd.DataFrame(
            [[1 / n_assets] * n_assets] * len(rebal_index),
            index=rebal_index,
            columns=df_close.columns
        )

        individual_port_val_df = calculate_portvals(df_close, target_weight_df)
        df_port = pd.DataFrame({'port': individual_port_val_df.sum(axis=1)})
        df_all = pd.concat([df_port, df_index], axis=1).dropna().rebase()

        # âœ… ì‹œê°í™”ë¥¼ ìœ„í•œ ì €ì¥
        st.session_state["latest_history_chart"] = df_all

        return df_all.tail().to_markdown()

    except Exception as e:
        return f"â— ë°±í…ŒìŠ¤íŠ¸ ë„ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
def plot_history_chart() -> str:
    """
    ê°€ì¥ ìµœê·¼ì— ì¡°íšŒí•œ ì£¼ê°€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì°¨íŠ¸ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    (get_yf_stock_history ì´í›„ì— í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤)
    """
    df = st.session_state.get("latest_history_chart")

    if df is None or df.empty:
        return "â— ì‹œê°í™”í•  ì£¼ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € get_yf_stock_historyë¥¼ í˜¸ì¶œí•´ì£¼ì„¸ìš”."

    st.subheader("ğŸ“ˆ ì£¼ê°€ íˆìŠ¤í† ë¦¬ ì°¨íŠ¸")
    st.line_chart(df, use_container_width=True)
    return "âœ… ì£¼ê°€ íˆìŠ¤í† ë¦¬ ì°¨íŠ¸ ì‹œê°í™” ì™„ë£Œ"


@tool
def get_backtest_summary_tool(ticker_list: list[str], period: str = "5y") -> str:
    """
    í¬íŠ¸í´ë¦¬ì˜¤ ì„±ê³¼ì§€í‘œ ìš”ì•½ (Total Return, CAGR, Sharpe, MDD, Volatility, 1M/3M/6M/YTD/1Y ìˆ˜ìµë¥ )
    """
    try:
        df_close = get_yf_close_prices(ticker_list, period)
        df_index = get_yf_close_prices(["SPY", "QQQ"], period)

        rebal_dates = get_rebal_dates(df_close, 'month')
        n_assets = len(df_close.columns)

        weight_df = pd.DataFrame(
            [[1 / n_assets] * n_assets] * len(rebal_dates),
            index=rebal_dates,
            columns=df_close.columns
        )

        portval_df = calculate_portvals(df_close, weight_df)
        port_series = portval_df.sum(axis=1)

        df_all = pd.concat([port_series.rename("port"), df_index], axis=1).dropna()
        rtn = get_returns_df(df_all, log=True)
        cum_rtn = get_cum_returns_df(rtn, log=True)

        today = df_all.index[-1]
        years = (today - df_all.index[0]).days / 365

        def get_cagr(series):
            return (series.iloc[-1] / series.iloc[0]) ** (1 / years) - 1

        def get_sharpe(series, rf=0.025):
            excess = series.mean() * 252 - rf
            return excess / (series.std() * np.sqrt(252))

        def get_mdd(series):
            cummax = series.cummax()
            dd = series / cummax - 1
            return dd.min()

        def get_weekly_vol(series):
            weekly_rtn = series.resample("W").last().pct_change().dropna()
            return weekly_rtn.std() * np.sqrt(52)

        def get_return_since(series, from_date):
            from_date = pd.to_datetime(from_date)
            if from_date not in series.index:
                from_date = series.index[series.index.get_indexer([from_date], method='nearest')[0]]
            return (series.loc[series.index[-1]] / series.loc[from_date]) - 1

        summary_data = []
        for col in df_all.columns:
            s_cum = cum_rtn[col]
            s_log = rtn[col]
            s_price = df_all[col]

            total_rtn = s_cum.iloc[-1] - 1
            cagr = get_cagr(s_cum)
            sharpe = get_sharpe(s_log)
            mdd = get_mdd(s_cum)
            vol = get_weekly_vol(s_price)

            one_month = today - pd.Timedelta(days=30)
            three_month = today - pd.Timedelta(days=90)
            six_month = today - pd.Timedelta(days=180)
            one_year = today - pd.Timedelta(days=365)
            ytd_start = pd.Timestamp(year=today.year, month=1, day=1)

            r_1m = get_return_since(s_cum, one_month)
            r_3m = get_return_since(s_cum, three_month)
            r_6m = get_return_since(s_cum, six_month)
            r_1y = get_return_since(s_cum, one_year)
            r_ytd = get_return_since(s_cum, ytd_start)

            summary_data.append([
                f"{total_rtn * 100:.2f}%",
                f"{cagr * 100:.2f}%",
                f"{sharpe:.2f}",
                f"{mdd * 100:.2f}%",
                f"{vol * 100:.2f}%",
                f"{r_1m * 100:.2f}%",
                f"{r_3m * 100:.2f}%",
                f"{r_6m * 100:.2f}%",
                f"{r_ytd * 100:.2f}%",
                f"{r_1y * 100:.2f}%",
            ])

        summary_df = pd.DataFrame(
            summary_data,
            columns=["Total Return", "CAGR", "Sharpe", "Max Drawdown", "Volatility (Weekly)",
                     "1M", "3M", "6M", "YTD", "1Y"],
            index=df_all.columns
        )

        return "ğŸ“Š ë°±í…ŒìŠ¤íŠ¸ ì„±ê³¼ ìš”ì•½:\n\n" + summary_df.to_markdown()

    except Exception as e:
        return f"â— ì„±ê³¼ ìš”ì•½ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}"

@tool
def get_naver_news_sentiment(company_name: str, display: int = 5) -> str:
    """
    NAVER ë‰´ìŠ¤ì—ì„œ ê¸°ì—… ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰ â†’ GPTë¡œ ê´€ë ¨ì„± í•„í„°ë§ + í•œê¸€ ìš”ì•½ + ê°ì„± ë¶„ì„.
    ê²°ê³¼ëŠ” ê¸°ì‚¬ ì œëª©, ë§í¬, ìš”ì•½, ê°ì„±ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
    """
    try:
        # 1. NAVER ë‰´ìŠ¤ API í˜¸ì¶œ
        client_id = os.getenv("NAVER_CLIENT_ID")
        client_secret = os.getenv("NAVER_CLIENT_SECRET")
        query = urllib.parse.quote(company_name)
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display={display}&sort=date"
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }

        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            return f"â— NAVER ë‰´ìŠ¤ API í˜¸ì¶œ ì˜¤ë¥˜: {response.status_code}"

        items = response.json().get("items", [])
        if not items:
            return f"â— '{company_name}' ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."

        # 2. ê¸°ì‚¬ ì •ì œ
        def clean(text):
            return html.unescape(BeautifulSoup(text, "html.parser").get_text())

        articles = []
        for i, item in enumerate(items):
            title = clean(item.get("title", "ì œëª© ì—†ìŒ"))
            desc = clean(item.get("description", ""))
            link = item.get("originallink") or item.get("link") or f"https://search.naver.com/search.naver?query={query}"
            articles.append(f"{i+1}. ì œëª©: {title}\në‚´ìš©: {desc}\në§í¬: {link}")

        news_text = "\n\n".join(articles)

        # 3. GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
'{company_name}'ì´ë¼ëŠ” í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ëœ ë‰´ìŠ¤ ëª©ë¡ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ì¤‘ ì¼ë¶€ëŠ” ê´€ë ¨ ì—†ëŠ” ê¸°ì‚¬ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤
(ì˜ˆ: ì• í”Œë¯¼íŠ¸, ì• í”Œìš°ë“œ ë“±). ì•„ë˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”:

1. ì‹¤ì œ ê¸°ì—… '{company_name}'ê³¼ ê´€ë ¨ ìˆëŠ” ê¸°ì‚¬ë§Œ ê³¨ë¼ì£¼ì„¸ìš”.
2. ê° ê¸°ì‚¬ë¥¼ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš” (ë³´ë„ë¬¸ ìŠ¤íƒ€ì¼ë¡œ).
3. ê° ê¸°ì‚¬ë³„ë¡œ 'ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½' ì¤‘ ê°ì„±ì„ íŒë‹¨í•´ì£¼ì„¸ìš”.
4. ê° ê¸°ì‚¬ì— ì œëª©/ë§í¬/ìš”ì•½/ê°ì„±ì„ í¬í•¨í•´ì£¼ì„¸ìš”.

[ë‰´ìŠ¤ ëª©ë¡]
{news_text}

ğŸ’¬ ì¶œë ¥ ì˜ˆì‹œ:
### 1. AI íˆ¬ì í™•ëŒ€  
ë§í¬: https://example.com/apple-ai  
ìš”ì•½: ...  
ê°ì„±: ê¸ì •
"""
        gpt_response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )

        return gpt_response.choices[0].message.content.strip()

    except Exception as e:
        return f"â— ë‰´ìŠ¤ ìš”ì•½ ë˜ëŠ” ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
@tool
def get_google_trend_tool(keyword: str, geo: str = "world") -> str:
    """
    Google ê²€ìƒ‰ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ëŸ‰ ë³€í™”ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    - keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë˜ëŠ” ê¸°ì—…ëª…
    - geo: êµ­ê°€ ì½”ë“œ (ì˜ˆ: 'KR', 'US', 'JP'; ê¸°ë³¸ê°’ 'world')
    """
    with no_ssl_verification():
        try:
            # 1. pytrends ì„¸íŒ…
            pytrends = TrendReq(hl="en-US", tz=360)
            geo_code = "" if geo.lower() == "world" else geo.upper()

            # 2. íŠ¸ë Œë“œ ìš”ì²­
            pytrends.build_payload([keyword], cat=0, timeframe="today 3-m", geo=geo_code, gprop="")

            data = pytrends.interest_over_time()
            if data.empty:
                return f"â— '{keyword}'ì— ëŒ€í•œ ê²€ìƒ‰ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            trend_series = data[keyword]

            # 3. Streamlit ì°¨íŠ¸ ì¶œë ¥
            st.subheader(f"ğŸ” ìµœê·¼ 3ê°œì›” Google ê²€ìƒ‰ íŠ¸ë Œë“œ: '{keyword}' ({geo_code or 'Global'})")
            st.line_chart(trend_series)

            # 4. ìš”ì•½ ë¶„ì„
            recent = trend_series.iloc[-1]
            peak = trend_series.max()
            avg = trend_series.mean()
            min_ = trend_series.min()

            result = (
                f"âœ… '{keyword}'ì— ëŒ€í•œ ìµœê·¼ 3ê°œì›”ê°„ ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„:\n"
                f"- ğŸ”¼ ìµœê³ ì¹˜: {peak:.0f}\n"
                f"- ğŸ”½ ìµœì €ì¹˜: {min_:.0f}\n"
                f"- ğŸ“Š í‰ê· : {avg:.1f}\n"
                f"- ğŸ“… ìµœê·¼ ê²€ìƒ‰ëŸ‰ (ë§ˆì§€ë§‰ì¼ ê¸°ì¤€): {recent:.0f}"
            )
            return result

        except Exception as e:
            return f"â— Google íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

@tool
def compare_google_trend_tool(keywords: list[str], geo: str = "world") -> str:
    """
    ì—¬ëŸ¬ í‚¤ì›Œë“œì— ëŒ€í•œ Google ê²€ìƒ‰ íŠ¸ë Œë“œë¥¼ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.
    - keywords: ê²€ìƒ‰ì–´ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['Apple', 'Microsoft'])
    - geo: êµ­ê°€ ì½”ë“œ (ê¸°ë³¸: 'world' â†’ ì „ì„¸ê³„, ì˜ˆ: 'KR', 'US')
    """
    with no_ssl_verification():
        try:
            if not keywords or len(keywords) < 2:
                return "â— 2ê°œ ì´ìƒì˜ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

            pytrends = TrendReq(hl="en-US", tz=360)
            geo_code = "" if geo.lower() == "world" else geo.upper()

            pytrends.build_payload(keywords, cat=0, timeframe="today 3-m", geo=geo_code, gprop="")

            data = pytrends.interest_over_time()
            if data.empty:
                return f"â— '{', '.join(keywords)}'ì— ëŒ€í•œ ê²€ìƒ‰ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            data = data.drop(columns=["isPartial"], errors="ignore")

            # âœ… ì°¨íŠ¸ ì¶œë ¥
            st.subheader(f"ğŸ” ìµœê·¼ 3ê°œì›” ê²€ìƒ‰ íŠ¸ë Œë“œ ë¹„êµ: {', '.join(keywords)}")
            st.line_chart(data)

            # âœ… ìš”ì•½ ë©”ì‹œì§€ ìƒì„±
            summaries = []
            for kw in keywords:
                recent = data[kw].iloc[-1]
                peak = data[kw].max()
                avg = data[kw].mean()
                summaries.append(
                    f"- **{kw}** â†’ ğŸ”¼ ìµœê³ : {peak:.0f}, ğŸ“Š í‰ê· : {avg:.1f}, ğŸ“… ìµœê·¼: {recent:.0f}"
                )

            return "âœ… Google ê²€ìƒ‰ íŠ¸ë Œë“œ ë¹„êµ ê²°ê³¼:\n" + "\n".join(summaries)

        except Exception as e:
            return f"â— Google íŠ¸ë Œë“œ ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


from langchain_core.tools import tool
import os
import requests
import urllib.parse
from bs4 import BeautifulSoup
import html
import pandas as pd
from datetime import datetime
import torch  # ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
from transformers import pipeline


# SSL ì¸ì¦ì„œ ìš°íšŒ
@tool
def get_company_news_with_sentiment(company_name: str, display: int = 10) -> str:
    """
    Naver ë‰´ìŠ¤ APIë¡œ ê¸°ì—… ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰ í›„ ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    import warnings
    import contextlib
    from urllib3.exceptions import InsecureRequestWarning
    old_merge_environment_settings = requests.Session.merge_environment_settings

    @contextlib.contextmanager
    def no_ssl_verification():
        opened_adapters = set()

        def merge_environment_settings(self, url, proxies, stream, verify, cert):
            opened_adapters.add(session.get_adapter(url))
            settings = old_merge_environment_settings(session, url, proxies, stream, verify, cert)
            settings['verify'] = False
            return settings

        session = requests.Session()
        requests.Session.merge_environment_settings = merge_environment_settings

        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore", InsecureRequestWarning)
                yield
        finally:
            requests.Session.merge_environment_settings = old_merge_environment_settings
            for adapter in opened_adapters:
                try:
                    adapter.close()
                except:
                    pass

    def clean(text):
        return html.unescape(BeautifulSoup(text, "html.parser").get_text())

    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")

    query = urllib.parse.quote(company_name)
    url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display={display}&sort=date"

    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }

    with no_ssl_verification():
        response = requests.get(url, headers=headers)

    if response.status_code != 200:
        return f"â— API ì˜¤ë¥˜: {response.status_code} - {response.text}"

    items = response.json().get("items", [])
    if not items:
        return f"â— '{company_name}' ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."

    # ë‰´ìŠ¤ ì •ë¦¬ ë° ê°ì„± ë¶„ì„
    news_list = []
    for item in items:
        title = clean(item.get("title", "ì œëª© ì—†ìŒ"))
        desc = clean(item.get("description", ""))
        link = item.get("originallink") or item.get("link") or f"https://search.naver.com/search.naver?query={query}"
        pub_date = item.get("pubDate", "")
        try:
            pub_date = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %z').strftime('%Y-%m-%d')
        except:
            pub_date = "ë‚ ì§œ ì˜¤ë¥˜"

        # ê°ì„± ë¶„ì„
        try:
            result = pipe(desc)[0]
            label = result["label"]
            score = result["score"]
        except Exception as e:
            label = "ë¶„ì„ ì˜¤ë¥˜"
            score = 0.0

        news_list.append(
            f"ğŸ“° [{title}]({link})\nğŸ“… {pub_date} | ê°ì„±: {label} ({score:.2f})\n{desc}\n"
        )

    return "\n\n".join(news_list[:display])


@tool
def fetch_wallstreetbets_posts_tool(limit: int = 50) -> list:
    """
    Redditì˜ r/wallstreetbetsì—ì„œ ê²Œì‹œê¸€ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    ìµœëŒ€ limit ê°œìˆ˜ê¹Œì§€ ìˆ˜ì§‘í•˜ë©°, ì œëª©, ë³¸ë¬¸, ì´ë¯¸ì§€ URL í¬í•¨.
    """
    reddit = praw.Reddit(
        client_id="RVKUBtrh7ExzRSbddfBDtg",
        client_secret="cT4m_YrJnZhZpZ2vPkVTfMT8hqj07A",
        user_agent="retail_stock_v1.0 (by /u/TraditionalIce9098)",
        check_for_async=False
    )
    reddit._core._requestor._http.verify = False

    def clean_submission(submission):
        data = {
            "title": submission.title,
            "selftext": submission.selftext,
            "url": f"https://www.reddit.com{submission.permalink}",
            "image_url": ""
        }
        # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ì¶”ì¶œ (ê°¤ëŸ¬ë¦¬ ë˜ëŠ” preview)
        try:
            if hasattr(submission, "preview") and "images" in submission.preview:
                data["image_url"] = submission.preview["images"][0]["source"]["url"].replace("&amp;", "&")
        except:
            pass
        return data

    with no_ssl_verification():
        subreddit = reddit.subreddit("wallstreetbets")
        posts = [clean_submission(post) for post in subreddit.hot(limit=limit)]
        return posts
    
    
@tool
def analyze_reddit_content_tool(title: str, selftext: str = "", image_url: str = "") -> dict:
    """
    Reddit ê²Œì‹œê¸€ ë‚´ìš©ì„ ì¢…í•© ë¶„ì„í•˜ì—¬ ê´€ë ¨ ì¢…ëª©, ìš”ì•½, êµ¬ë¶„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if image_url:
        try:
            img_prompt = "ë‹¤ìŒ ì´ë¯¸ì§€ëŠ” wallstreetbetsì— ì˜¬ë¼ì˜¨ íˆ¬ì ê´€ë ¨ ë°ˆì´ì•¼. ì´ ì´ë¯¸ì§€ì—ì„œ ì–´ë–¤ íˆ¬ì ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ë ¤ëŠ”ì§€ í•´ì„í•´ì¤˜."
            img_response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": img_prompt},
                        {"type": "image_url", "image_url": {"url": image_url}},
                    ]
                }]
            )
            image_analysis = img_response.choices[0].message.content.strip()
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}")
            image_analysis = "ì´ë¯¸ì§€ ì—†ìŒ"
    else:
        image_analysis = "ì´ë¯¸ì§€ ì—†ìŒ"

    system_msg = {
        "role": "system",
        "content": """
wallstreetbetsì— ì˜¬ë¼ì˜¨ íˆ¬ì ê´€ë ¨ ê²Œì‹œë¬¼ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
ì œëª©, ë³¸ë¬¸, ì´ë¯¸ì§€ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê²Œì‹œë¬¼ ì„±ê²©ê³¼ íˆ¬ì ìš”ì•½, ì¢…ëª©(symbol)ì„ ì¶”ì¶œí•˜ì„¸ìš”.

ë‹¤ìŒ JSON í˜•íƒœë¡œ ë°˜í™˜:
{
  "êµ¬ë¶„": "íˆ¬ìì •ë³´ / íˆ¬ìí›„ê¸° / ì§ˆë¬¸ / ê¸°íƒ€",
  "ë‚´ìš©ìš”ì•½": "ë‚´ìš© í•µì‹¬ ìš”ì•½",
  "ê´€ë ¨ì¢…ëª©": "ì˜ˆ: TSLA, NVDA"
}
        """
    }

    user_msg = {
        "role": "user",
        "content": f"ì œëª©: {title}\në³¸ë¬¸: {selftext}\nì´ë¯¸ì§€ í•´ì„: {image_analysis}"
    }

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[system_msg, user_msg],
            temperature=0
        )
        content = response.choices[0].message.content.strip()
        content = content.replace('```json','').replace('```','')
        return json.loads(content)
    except Exception as e:
        print(f"ë³¸ë¬¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {
            "êµ¬ë¶„": "ë¶„ì„ì‹¤íŒ¨",
            "ë‚´ìš©ìš”ì•½": "",
            "ê´€ë ¨ì¢…ëª©": ""
        }


# ë„êµ¬ ë°”ì¸ë”©
# ----- ë„êµ¬ ë°”ì¸ë”© -----
tools = [
    get_current_time,
    get_yf_stock_info,
    get_yf_stock_history,
    get_yf_stock_recommendations,
    plot_history_chart,  # âœ… ì¶”ê°€
    get_backtest_tool,  # âœ… ì¶”ê°€
    get_backtest_summary_tool,
    get_naver_news_sentiment,  # âœ… NAVER ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ë„êµ¬,  # âœ… ì¶”ê°€
    get_google_trend_tool, 
    compare_google_trend_tool,  # âœ… ì¶”ê°€
    get_company_news_with_sentiment,
    fetch_wallstreetbets_posts_tool, 
    analyze_reddit_content_tool,
]

# nameì„ í‚¤ë¡œ í•˜ëŠ” dict ìƒì„±
tool_dict = {tool.name: tool for tool in tools}

llm_with_tools = llm.bind_tools(tools)


# ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
def get_ai_response(messages):
    response = llm_with_tools.stream(messages) # â‘  llm.stream()ì„ llm_with_tools.stream()ë¡œ ë³€ê²½
    
    gathered = None # â‘¡
    for chunk in response:
        yield chunk
        
        if gathered is None: #  â‘¢
            gathered = chunk
        else:
            gathered += chunk
 
    if gathered.tool_calls:
        st.session_state.messages.append(gathered)
        
        for tool_call in gathered.tool_calls:
            selected_tool = tool_dict[tool_call['name']]
            tool_msg = selected_tool.invoke(tool_call) 
            print(tool_msg, type(tool_msg))
            st.session_state.messages.append(tool_msg)
           
        for chunk in get_ai_response(st.session_state.messages):
            yield chunk


# Streamlit ì•±
st.title("ğŸ’¬ GPT-4o Stock Chain Bot")

with st.expander("ğŸ“Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ ìš”ì•½ ë³´ê¸°"):
    st.markdown("""
### ğŸ¯ ì§€ì› ê¸°ëŠ¥ ë° ì˜ˆì‹œ ëª…ë ¹ì–´

| ê¸°ëŠ¥ ì´ë¦„ | ì„¤ëª… | ğŸ” ì˜ˆì‹œ ëª…ë ¹ì–´ |
|-----------|------|----------------|
| `get_current_time` | ì§€ì •í•œ íƒ€ì„ì¡´ì˜ í˜„ì¬ ì‹œê°„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. | `"ì„œìš¸ì˜ í˜„ì¬ ì‹œê°„ì„ ì•Œë ¤ì¤˜"` |
| `get_yf_stock_info` | ì…ë ¥í•œ ì¢…ëª© í‹°ì»¤ì˜ Yahoo Finance ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. | `"AAPL ì¢…ëª© ì •ë³´ ì•Œë ¤ì¤˜"` |
| `get_yf_stock_history` | ì¢…ëª©ì˜ ì£¼ê°€ ì´ë ¥ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. | `"TSLAì˜ ìµœê·¼ ì£¼ê°€ íë¦„ ë³´ì—¬ì¤˜"` |
| `get_yf_stock_recommendations` | ì• ë„ë¦¬ìŠ¤íŠ¸ë“¤ì˜ ì¢…ëª© ì¶”ì²œ ë°ì´í„°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. | `"NVDAì— ëŒ€í•œ ë¦¬ì„œì¹˜ ì¶”ì²œ ë³´ì—¬ì¤˜"` |
| `plot_history_chart` | ì£¼ê°€ ì´ë ¥ì„ ì‹œê°í™”í•©ë‹ˆë‹¤. (ì´ì „ ì¡°íšŒ í•„ìš”) | `"ì°¨íŠ¸ë¡œ ë³´ì—¬ì¤˜"` |
| `get_backtest_tool` | í¬íŠ¸í´ë¦¬ì˜¤ ëˆ„ì  ìˆ˜ìµë¥  ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ | `"AAPLê³¼ MSFTë¡œ 5ë…„ ë°±í…ŒìŠ¤íŠ¸ í•´ì¤˜"` |
| `get_backtest_summary_tool` | í¬íŠ¸ ìˆ˜ìµë¥ , ë³€ë™ì„±, ìƒ¤í”„ì§€ìˆ˜ ë“± ìš”ì•½ | `"í…ŒìŠ¬ë¼ì™€ ì—”ë¹„ë””ì•„ë¡œ í¬íŠ¸ ì„±ê³¼ ìš”ì•½í•´ì¤˜"` |
| `get_naver_news_sentiment` | NAVER ë‰´ìŠ¤ì—ì„œ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰ + ê°ì„± ë¶„ì„ | `"ì‚¼ì„±ì „ì ê´€ë ¨ ë‰´ìŠ¤ ê°ì„± ë¶„ì„í•´ì¤˜"` |
| `get_company_news_with_sentiment` | Naver ë‰´ìŠ¤ì—ì„œ ê¸°ì‚¬ + ê°ì„± ì ìˆ˜ë¡œ ìš”ì•½ | `"LGì—ë„ˆì§€ì†”ë£¨ì…˜ ë‰´ìŠ¤ ê°ì„± ìš”ì•½"` |
| `get_google_trend_tool` | Google ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„ (ìµœê·¼ 3ê°œì›”) | `"ì• í”Œ ê²€ìƒ‰ íŠ¸ë Œë“œ ë³´ì—¬ì¤˜"` |
| `compare_google_trend_tool` | ì—¬ëŸ¬ í‚¤ì›Œë“œ ê²€ìƒ‰ëŸ‰ ë¹„êµ | `"ì‚¼ì„±ì „ìì™€ ì• í”Œ ê²€ìƒ‰ëŸ‰ ë¹„êµ"` |
| `fetch_wallstreetbets_posts_tool` | Redditì—ì„œ ì¸ê¸° ê²Œì‹œê¸€ ìˆ˜ì§‘ | `"wallstreetbetsì—ì„œ ìµœê·¼ ì¸ê¸° ê¸€ ê°€ì ¸ì™€ì¤˜"` |
| `analyze_reddit_content_tool` | Reddit ê²Œì‹œê¸€ ë¶„ì„ â†’ ì¢…ëª©, ìš”ì•½, ì„±ê²© ë¶„ë¥˜ | `"ì´ ê¸€ ë¶„ì„í•´ì„œ ê´€ë ¨ ì¢…ëª© ë½‘ì•„ì¤˜"` |
    """)


# ìŠ¤íŠ¸ë¦¼ë¦¿ session_stateì— ë©”ì‹œì§€ ì €ì¥
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        SystemMessage("ë„ˆëŠ” íˆ¬ììë¥¼ ë•ê¸° ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë´‡ì´ë‹¤."),  
        AIMessage("How can I help you?")
    ]

# ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    if msg.content:
        if isinstance(msg, SystemMessage):
            st.chat_message("system").write(msg.content)
        elif isinstance(msg, AIMessage):
            st.chat_message("assistant").write(msg.content)
        elif isinstance(msg, HumanMessage):
            st.chat_message("user").write(msg.content)
        elif isinstance(msg, ToolMessage):
            st.chat_message("tool").write(msg.content)


# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input():
    st.chat_message("user").write(prompt) # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
    st.session_state.messages.append(HumanMessage(prompt)) # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥

    response = get_ai_response(st.session_state["messages"])
    
    result = st.chat_message("assistant").write_stream(response) # AI ë©”ì‹œì§€ ì¶œë ¥
    st.session_state["messages"].append(AIMessage(result)) # AI ë©”ì‹œì§€ ì €ì¥    