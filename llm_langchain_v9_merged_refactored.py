import requests
old_merge_environment_settings = requests.Session.merge_environment_settings


from curl_cffi.requests import Session
import yfinance as yf
import pandas as pd
import numpy as np
import ffn
from langchain_core.tools import tool
# from duckduckgo_search import DDGS
from openai import OpenAI
import os
import html
import urllib.parse
from bs4 import BeautifulSoup
from googlesearch import search  # google ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÇ¨Ïö©
from pytrends.request import TrendReq
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from no_ssl_verification import no_ssl_verification
load_dotenv()
import time
from collections import Counter
import praw
# from no_ssl_verification import no_ssl_verification
# .env ÌååÏùºÏóêÏÑú ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú
load_dotenv()
import io
import warnings
import contextlib
import openai
import json 
from requests.auth import HTTPBasicAuth

import streamlit as st

from code_merge3 import get_all_us_tickers, extract_tickers_from_text, filter_relevant_tickers_with_gpt, get_detailed_info
from collections import Counter, defaultdict

import ssl
import re
from urllib3.exceptions import InsecureRequestWarning

# from duckduckgo_search import DDGS
load_dotenv()
# .env ÌååÏùºÏóêÏÑú ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú
load_dotenv()




def draw_backtest_chart(df_all: pd.DataFrame):
    """
    Î∞±ÌÖåÏä§Ìä∏ Í≤∞Í≥º(df_all)Î•º Streamlit Ï∞®Ìä∏Î°ú ÏãúÍ∞ÅÌôîÌï©ÎãàÎã§.
    """
    if df_all is None or df_all.empty:
        st.warning("üì≠ ÏãúÍ∞ÅÌôîÌï† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")
        return

    st.subheader("üìà ÎàÑÏ†Å ÏàòÏùµÎ•† ÎπÑÍµê")
    st.line_chart(df_all)
    st.success("‚úÖ Î∞±ÌÖåÏä§Ìä∏ ÎàÑÏ†Å ÏàòÏùµÎ•†Ïù¥ ÏãúÍ∞ÅÌôîÎêòÏóàÏäµÎãàÎã§.")
old_merge_environment_settings = requests.Session.merge_environment_settings


@contextlib.contextmanager
def no_ssl_verification():
    opened_adapters = set()
    def merge_environment_settings(self, url, proxies, stream, verify, cert):
        opened_adapters.add(self.get_adapter(url))
        settings = old_merge_environment_settings(self, url, proxies, stream, verify, cert)
        settings['verify'] = False
        return settings
    requests.Session.merge_environment_settings = merge_environment_settings
    try:
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', InsecureRequestWarning)
            yield
    finally:
        requests.Session.merge_environment_settings = old_merge_environment_settings
        for adapter in opened_adapters:
            try:
                adapter.close()
            except:
                pass

def run_reddit_summary(limit=20, subreddit="wallstreetbets") -> str:
    reddit = praw.Reddit(
        client_id="RVKUBtrh7ExzRSbddfBDtg",
        client_secret="cT4m_YrJnZhZpZ2vPkVTfMT8hqj07A",
        user_agent="retail_stock_v1.0 (by /u/TraditionalIce9098)",
        check_for_async=False
    )
    reddit._core._requestor._http.verify = False

    tickers_set = get_all_us_tickers()
    posts = list(reddit.subreddit(subreddit).hot(limit=limit))

    ticker_counter = Counter()
    ticker_posts = defaultdict(list)

    for post in posts:
        combined_text = (post.title or "") + " " + (post.selftext or "")
        raw_tickers = extract_tickers_from_text(combined_text, tickers_set)
        filtered = filter_relevant_tickers_with_gpt(post.title, post.selftext, raw_tickers)

        for ticker in filtered:
            ticker_counter[ticker] += 1
            ticker_posts[ticker].append({
                "title": post.title,
                "url": f"https://www.reddit.com{post.permalink}",
                "body": post.selftext
            })

    if not ticker_counter:
        return "‚ùó Ïú†Ìö®Ìïú Ï¢ÖÎ™© Ïñ∏Í∏âÏù¥ Ìè¨Ìï®Îêú Í≤åÏãúÎ¨ºÏù¥ ÏóÜÏäµÎãàÎã§."

    top = ticker_counter.most_common(10)
    result = "# üìä Reddit Ïù∏Í∏∞ Ï¢ÖÎ™© Î∂ÑÏÑù\n\n"
    for rank, (ticker, count) in enumerate(top, 1):
        info = get_detailed_info(ticker)
        if not info:
            continue
        symbol = "üìà" if info["change_pct"] > 0 else "üìâ" if info["change_pct"] < 0 else "‚û°Ô∏è"
        market_cap = info["market_cap"]
        mc = f"${market_cap:,}" if market_cap else "N/A"

        result += f"""
### {rank}. [{ticker}](https://finance.yahoo.com/quote/{ticker}) ‚Äî {count}Ìöå Ïñ∏Í∏â  
- üíµ ÌòÑÏû¨Í∞Ä: ${info['price']} ({symbol} {info['change_pct']}%)  
- üè∑Ô∏è ÏÑπÌÑ∞: {info['sector']} / {info['industry']}  
- üí∞ ÏãúÍ∞ÄÏ¥ùÏï°: {mc}  
- üìä PER: {info['pe_ratio'] or 'N/A'}  
- üîó Í¥ÄÎ†® Í≤åÏãúÍ∏Ä:\n"""

        for post in ticker_posts[ticker][:3]:
            preview = post["body"][:100].replace("\n", " ") + "‚Ä¶" if post["body"] else ""
            result += f"  - [{post['title']}]({post['url']}) ‚Äî {preview}\n"

        result += "\n"
    return result.strip()


load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')

# Reddit Ïù∏Ï¶ù Ï†ïÎ≥¥
reddit_client_id = "RVKUBtrh7ExzRSbddfBDtg"
reddit_client_secret = "cT4m_YrJnZhZpZ2vPkVTfMT8hqj07A"
reddit_user_agent = "retail_stock_v1.0 (by /u/TraditionalIce9098)"

# Reddit ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉùÏÑ±
reddit = praw.Reddit(
    client_id=reddit_client_id,
    client_secret=reddit_client_secret,
    user_agent=reddit_user_agent,
    check_for_async=False
)

# GPTÎ•º ÏÇ¨Ïö©Ìï¥ Í¥ÄÎ†® Ï¢ÖÎ™© Ï∂îÏ∂ú
def extract_related_tickers(title, body, model="gpt-4o"):
    prompt = f"""
Îã§Ïùå Reddit Í≤åÏãúÍ∏Ä Ï†úÎ™©, Î≥∏Î¨∏, Ïù¥ÎØ∏ÏßÄ Îì±ÏóêÏÑú Ïñ∏Í∏âÎêú Ï£ºÏöî Ï£ºÏãù Ï¢ÖÎ™©(symbol)ÏùÑ ÏµúÎåÄ 3Í∞úÍπåÏßÄ Ï∂îÏ∂úÌïòÏÑ∏Ïöî.
Ï£ºÏãùÏùÄ Ìã∞Ïª§Î°ú Ïñ∏Í∏âÎê† ÏàòÎèÑ ÏûàÏßÄÎßå, Ï¢ÖÎ™©Î™Ö(Ïòà: Microsoft;MSFT)ÏúºÎ°ú Ïñ∏Í∏âÎê† ÏàòÎèÑ ÏûàÏäµÎãàÎã§.
ÎØ∏Íµ≠ Ï£ºÏãù Í∏∞Ï§ÄÏúºÎ°ú Ï¢ÖÎ™© ÏΩîÎìú(TSLA, GME Îì±)Î•º Î∞òÌôòÌïòÍ≥†, ÏóÜÏúºÎ©¥ Îπà Î¶¨Ïä§Ìä∏Î•º Î∞òÌôòÌïòÏÑ∏Ïöî.

Ï†úÎ™©: {title}
Î≥∏Î¨∏: {body[:1000]}

Í≤∞Í≥ºÎäî JSON Î∞∞Ïó¥ ÌòïÏãùÏúºÎ°úÎßå Ï£ºÏÑ∏Ïöî. Ïòà: ["TSLA", "AAPL"]
"""
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful financial assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
        )
        content = response.choices[0].message.content.strip()
        tickers = eval(content) if content.startswith("[") else []
        return tickers
    except Exception as e:
        print(f"‚ùå GPT Ïò§Î•ò: {e}")
        return []

llm = ChatOpenAI(model = 'gpt-4o')



openai_api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=openai_api_key)

session = Session(
        impersonate="chrome110",  # optional, mimics Chrome browser
        headers={
            "User-Agent": "Mozilla/5.0"
        },
        verify=False  # optional; disable SSL verify if you're behind a proxy
    )
def get_yf_close_prices(tickers: list[str], period: str) -> pd.DataFrame:
    ticker_objs = yf.Tickers(" ".join(tickers),  session= session)
    history_dict = {
        symbol: ticker_objs.tickers[symbol].history(period=period)
        for symbol in tickers
    }

    # Í∞Å Ìã∞Ïª§Ïùò 'Close' ÏãúÎ¶¨Ï¶àÎ•º Î™®ÏïÑ ÌïòÎÇòÏùò DataFrameÏúºÎ°ú
    close_df = pd.DataFrame({
        symbol: df["Close"]
        for symbol, df in history_dict.items()
        if "Close" in df.columns
    })
    close_df.index = close_df.index.strftime("%Y-%m-%d")
    close_df.index = pd.DatetimeIndex(close_df.index)
    return close_df

def get_returns_df(df, N=1, log=False):
    if log:
        return np.log(df / df.shift(N)).iloc[N-1:].fillna(0)
    else:
        return df.pct_change(N, fill_method=None).iloc[N-1:].fillna(0)

def get_cum_returns_df(return_df, log=False):
    if log:
        return np.exp(return_df.cumsum())
    else:
        return (1 + return_df).cumprod()    # same with (return_df.cumsum() + 1)

def get_CAGR_series(cum_rtn_df, num_day_in_year=250):
    cagr_series = cum_rtn_df.iloc[-1]**(num_day_in_year/(len(cum_rtn_df))) - 1
    return cagr_series

def get_sharpe_ratio(log_rtn_df, yearly_rfr = 0.025):
    excess_rtns = log_rtn_df.mean()*252 - yearly_rfr
    return excess_rtns / (log_rtn_df.std() * np.sqrt(252))

def get_drawdown_infos(cum_returns_df): 
    # 1. Drawdown
    cummax_df = cum_returns_df.cummax()
    dd_df = cum_returns_df / cummax_df - 1
 
    # 2. Maximum drawdown
    mdd_series = dd_df.min()

    # 3. longest_dd_period
    dd_duration_info_list = list()
    max_point_df = dd_df[dd_df == 0]
    for col in max_point_df:
        _df = max_point_df[col]
        _df.loc[dd_df[col].last_valid_index()] = 0
        _df = _df.dropna()

        periods = _df.index[1:] - _df.index[:-1]

        days = periods.days
        max_idx = days.argmax()

        longest_dd_period = days.max()
        dd_mean = int(np.mean(days))
        dd_std = int(np.std(days))

        dd_duration_info_list.append(
            [
                dd_mean,
                dd_std,
                longest_dd_period,
                "{} ~ {}".format(_df.index[:-1][max_idx].date(), _df.index[1:][max_idx].date())
            ]
        )

    dd_duration_info_df = pd.DataFrame(
        dd_duration_info_list,
        index=dd_df.columns,
        columns=['drawdown mean', 'drawdown std', 'longest days', 'longest period']
    )
    return dd_df, mdd_series, dd_duration_info_df

def get_rebal_dates(price_df, period="month"):
    _price_df = price_df.reset_index()
    if period == "month":
         groupby = [_price_df['Date'].dt.year, _price_df['Date'].dt.month]
    elif period == "quarter":
        groupby = [_price_df['Date'].dt.year, _price_df['Date'].dt.quarter]
    elif period == "halfyear":
        groupby = [_price_df['Date'].dt.year, _price_df['Date'].dt.month // 7]
    elif period == "year":
        groupby = [_price_df['Date'].dt.year, _price_df['Date'].dt.year]
    rebal_dates = pd.to_datetime(_price_df.groupby(groupby)['Date'].last().values)
    return rebal_dates

from functools import reduce

def calculate_portvals(price_df, weight_df):
    cum_rtn_up_until_now = 1 
    individual_port_val_df_list = []

    prev_end_day = weight_df.index[0]
    for end_day in weight_df.index[1:]:
        sub_price_df = price_df.loc[prev_end_day:end_day]
        sub_asset_flow_df = sub_price_df / sub_price_df.iloc[0]

        weight_series = weight_df.loc[prev_end_day]
        indi_port_cum_rtn_series = (sub_asset_flow_df * weight_series) * cum_rtn_up_until_now
    
        individual_port_val_df_list.append(indi_port_cum_rtn_series)

        total_port_cum_rtn_series = indi_port_cum_rtn_series.sum(axis=1)
        cum_rtn_up_until_now = total_port_cum_rtn_series.iloc[-1]

        prev_end_day = end_day 

    individual_port_val_df = reduce(lambda x, y: pd.concat([x, y.iloc[1:]]), individual_port_val_df_list)
    return individual_port_val_df

def get_backtest(ticker_list, period:str = '5y'):
    # RedditÏóêÏÑú ticker get
    tickers = ticker_list
    df_close = get_yf_close_prices(tickers, period)
    index = ['SPY', 'QQQ']
    df_index = get_yf_close_prices(index, "5y")

    rebal_dates = get_rebal_dates(df_close, 'month')
    rebal_index = rebal_dates
    result_portval_dict = {} 
    n_assets = df_close.shape[1]
    target_ratios = np.array([1/n_assets] * n_assets)

    target_weight_df = pd.DataFrame(
        [[1/len(df_close.columns)]*len(df_close.columns)]* len(rebal_index),
        index=rebal_index,
        columns=df_close.columns
    )

    cum_rtn_at_last_month_end = 1
    individual_port_val_df_list = []

    individual_port_val_df = calculate_portvals(df_close, target_weight_df)
    individual_port_val_df.head()
    result_portval_dict['port'] = individual_port_val_df.sum(axis=1)

    import ffn
    pd.concat([pd.DataFrame(result_portval_dict), df_index], axis = 1).dropna().rebase().plot();
    df_all = pd.concat([pd.DataFrame(result_portval_dict), df_index], axis = 1).dropna().rebase()

        # Íµ¨Í∞ÑÎ≥Ñ
    stats = df_all.rebase().calc_stats()
    stats.display()
    # stats.stats.to_clipboard()

    # qs.reports.full(df_all.loc[:,'port'], df_all.loc[:, 'SPY']).to_markdown() 
    return df_all.to_markdown()

from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
# from graph_chart import draw_backtest_chart
from datetime import datetime
import pytz
from langchain_core.messages import HumanMessage
import streamlit.components.v1 as components
load_dotenv()



# ÎØ∏Íµ≠ Ï†ÑÏ≤¥ ÏÉÅÏû• Ï¢ÖÎ™© ÏûêÎèô Î°úÎî©
@st.cache_data
def get_all_us_tickers():
    with no_ssl_verification():
        nasdaq_url = "https://www.nasdaqtrader.com/dynamic/SymDir/nasdaqlisted.txt"
        other_url = "https://www.nasdaqtrader.com/dynamic/SymDir/otherlisted.txt"

        nasdaq_df = pd.read_csv(io.StringIO(requests.get(nasdaq_url).text), sep="|")
        other_df = pd.read_csv(io.StringIO(requests.get(other_url).text), sep="|")

        nasdaq_df = nasdaq_df[~nasdaq_df.iloc[:, 0].str.contains("File Creation Time", na=False)]
        other_df = other_df[~other_df.iloc[:, 0].str.contains("File Creation Time", na=False)]

        nasdaq_col = next((col for col in nasdaq_df.columns if "Symbol" in col), None)
        other_col = next((col for col in other_df.columns if "ACT Symbol" in col), None)

        if not nasdaq_col or not other_col:
            raise ValueError("‚ùå Ìã∞Ïª§ Ïª¨ÎüºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")

        tickers = pd.concat([nasdaq_df[nasdaq_col], other_df[other_col]]).dropna().unique().tolist()
        return set(tickers)

# Ï¢ÖÎ™© Ï∂îÏ∂ú, ÏãúÍ∞ÄÏ¥ùÏï° Ìè¨Îß∑, Ï£ºÍ∞Ä Ï†ïÎ≥¥ ÏàòÏßë
def extract_tickers_from_text(text, valid_tickers):
    words = re.findall(r'\b[A-Z]{1,5}\b', text.upper())
    return [word for word in words if word in valid_tickers]


def filter_relevant_tickers_with_gpt(title: str, body: str, tickers: list) -> list:
    """
    Ï∂îÏ∂úÎêú ticker Ï§ë Ïã§Ï†ú Ìà¨Ïûê Í¥ÄÎ†® Îß•ÎùΩÏóêÏÑú Ïñ∏Í∏âÎêú Í≤ÉÎßå GPTÎ°ú ÌïÑÌÑ∞ÎßÅ
    """
    if not tickers:
        return []

    system_msg = {
        "role": "system",
        "content": """Îã§ÏùåÏùÄ RedditÏùò Ìà¨Ïûê Í¥ÄÎ†® Í≤åÏãúÍ∏ÄÏûÖÎãàÎã§.
Ï†úÎ™©Í≥º Î≥∏Î¨∏ÏùÑ ÏùΩÍ≥†, Ï†úÏãúÎêú Ï¢ÖÎ™©Î™Ö Î¶¨Ïä§Ìä∏ Ï§ë Ïã§Ï†ú Ìà¨Ïûê Îß•ÎùΩÏóêÏÑú Ïñ∏Í∏âÎêú Ï¢ÖÎ™©Îßå ÌïÑÌÑ∞ÎßÅÌïòÏÑ∏Ïöî.

Îã§ÏùåÍ≥º Í∞ôÏùÄ ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌïòÏÑ∏Ïöî:
["TSLA", "NVDA"]
"""
    }

    user_msg = {
        "role": "user",
        "content": f"Ï†úÎ™©: {title}\nÎ≥∏Î¨∏: {body}\nÏ¢ÖÎ™© Î¶¨Ïä§Ìä∏: {tickers}"
    }

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[system_msg, user_msg],
            temperature=0
        )
        result = response.choices[0].message.content.strip()
        return json.loads(result)
    except Exception as e:
        print(f"GPT ÌïÑÌÑ∞ÎßÅ Ïã§Ìå®: {e}")
        return []

def format_market_cap(value):
    if not value:
        return "N/A"
    for suffix, factor in [("T", 1e12), ("B", 1e9), ("M", 1e6), ("K", 1e3)]:
        if value >= factor:
            return f"${value / factor:.2f}{suffix}"
    return f"${value:.2f}"

def get_detailed_info(ticker):
    try:
        stock = yf.Ticker(ticker, session= session)
        info = stock.info
        data = stock.history(period="2d")
        if data.shape[0] < 2:
            return None
        price = data["Close"].iloc[-1]
        prev_price = data["Close"].iloc[-2]
        change_pct = ((price - prev_price) / prev_price) * 100
        return {
            "ticker": ticker,
            "name": info.get("longName", "N/A"),
            "sector": info.get("sector", "N/A"),
            "industry": info.get("industry", "N/A"),
            "price": round(price, 2),
            "change_pct": round(change_pct, 2),
            "market_cap": info.get("marketCap", None),
            "pe_ratio": info.get("trailingPE", None)
        }
    except Exception:
        return None







llm = ChatOpenAI(model = 'gpt-4o')

# ÎèÑÍµ¨ Ìï®Ïàò Ï†ïÏùò
@tool
def get_current_time(timezone: str, location: str) -> str:
    """ÌòÑÏû¨ ÏãúÍ∞ÅÏùÑ Î∞òÌôòÌïòÎäî Ìï®Ïàò."""
    try:
        tz = pytz.timezone(timezone)
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        result = f'{timezone} ({location}) ÌòÑÏû¨ÏãúÍ∞Å {now}'
        print(result)
        return result
    except pytz.UnknownTimeZoneError:
        return f"Ïïå Ïàò ÏóÜÎäî ÌÉÄÏûÑÏ°¥: {timezone}"

@tool
def get_yf_stock_history(ticker: str, period: str) -> str:
    """
    Ï¢ÖÎ™©Ïùò Ï£ºÍ∞Ä Ïù¥Î†•ÏùÑ Ï°∞ÌöåÌï¥ ÌÖçÏä§Ìä∏Î°ú Î∞òÌôòÌï©ÎãàÎã§.
    ÏãúÍ∞ÅÌôîÎäî Î≥ÑÎèÑ ÎèÑÍµ¨ÏóêÏÑú Ï≤òÎ¶¨Ìï©ÎãàÎã§.
    """
    stock = yf.Ticker(ticker, session= session)
    df = stock.history(period=period)

    if df.empty:
        return f"{ticker}Ïùò {period} Í∏∞Í∞Ñ Ï£ºÍ∞Ä Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§."

    # ‚úÖ ÏãúÍ∞ÅÌôîÎ•º ÏúÑÌï¥ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
    st.session_state["latest_history_chart"] = df[["Close"]].copy()
    st.session_state["latest_history_chart"].index = st.session_state["latest_history_chart"].index.strftime("%Y-%m-%d")

    return df.tail().to_markdown()


@tool
def get_yf_cumulative_returns_tool(ticker_list: list, period: str = "3mo") -> str:
    """
    Ïó¨Îü¨ Ï¢ÖÎ™©Ïùò ÎàÑÏ†Å ÏàòÏùµÎ•† (Cumulative Returns)ÏùÑ Í≥ÑÏÇ∞ÌïòÏó¨ ÎπÑÍµê ÌëúÎ°ú Î∞òÌôòÌï©ÎãàÎã§.
    """
    price_df = pd.DataFrame()

    for ticker in ticker_list:
        try:
            df = yf.Ticker(ticker, session=session).history(period=period)
            if df.empty:
                continue
            price_df[ticker] = df["Close"]
        except Exception as e:
            continue

    if price_df.empty:
        return "‚ùó Ïú†Ìö®Ìïú Ï¢ÖÎ™© Í∞ÄÍ≤© Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§."

    # ÎàÑÏ†Å ÏàòÏùµÎ•† Í≥ÑÏÇ∞
    rtn_df = price_df.pct_change().fillna(0)
    cum_rtn_df = (1 + rtn_df).cumprod()

    # ÎßàÏßÄÎßâ Í∏∞Ï§Ä ÏàòÏùµÎ•† ÏöîÏïΩ
    final_returns = (cum_rtn_df.iloc[-1] - 1).sort_values(ascending=False) * 100
    summary_df = final_returns.to_frame(name="ÎàÑÏ†Å ÏàòÏùµÎ•† (%)")
    summary_df.index.name = "Ï¢ÖÎ™©"

    # ‚úÖ ÏÑ∏ÏÖò ÏÉÅÌÉú Ï†ÄÏû• (Ï∞®Ìä∏Ïö©)
    st.session_state["latest_cum_rtn_df"] = cum_rtn_df.copy()

    return f"üìä {period} Í∏∞Í∞Ñ ÎàÑÏ†Å ÏàòÏùµÎ•† ÎπÑÍµê\n\n" + summary_df.round(2).to_markdown()





@tool
def get_yf_stock_info(ticker: str) -> str:
    """Ìï¥Îãπ Ï¢ÖÎ™©Ïùò Yahoo Finance Ï†ïÎ≥¥Î•º Î∞òÌôòÌï©ÎãàÎã§."""
    stock = yf.Ticker(ticker, session= session)
    info = stock.info
    return str(info)

@tool
def get_yf_stock_recommendations(ticker: str) -> str:
    """Ìï¥Îãπ Ï¢ÖÎ™©Ïùò Î¶¨ÏÑúÏπò Ï∂îÏ≤ú Ï†ïÎ≥¥Î•º Î∞òÌôòÌï©ÎãàÎã§."""
    stock = yf.Ticker(ticker, session= session)
    recommendations = stock.recommendations
    if recommendations is None or recommendations.empty:
        return f"{ticker}Ïóê ÎåÄÌïú Ï∂îÏ≤ú Î¶¨ÏÑúÏπò Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§."
    return recommendations.to_markdown()

@tool
def get_backtest_tool(ticker_list: list[str], period: str = "5y") -> str:
    """
    Ï£ºÏñ¥ÏßÑ Ï¢ÖÎ™© Î¶¨Ïä§Ìä∏ÏôÄ Í∏∞Í∞ÑÏúºÎ°ú Î∞±ÌÖåÏä§Ìä∏Î•º Ïã§ÌñâÌïòÍ≥†,
    ÎàÑÏ†Å ÏàòÏùµÎ•† Ï∞®Ìä∏Î•º ÏãúÍ∞ÅÌôîÌïòÎ©∞, ÏöîÏïΩ Î¶¨Ìè¨Ìä∏Î•º Î∞òÌôòÌï©ÎãàÎã§.
    """
    try:
        df_close = get_yf_close_prices(ticker_list, period)
        index = ['SPY', 'QQQ']
        df_index = get_yf_close_prices(index, "5y")

        rebal_dates = get_rebal_dates(df_close, 'month')
        rebal_index = rebal_dates

        n_assets = df_close.shape[1]
        target_weight_df = pd.DataFrame(
            [[1 / n_assets] * n_assets] * len(rebal_index),
            index=rebal_index,
            columns=df_close.columns
        )

        individual_port_val_df = calculate_portvals(df_close, target_weight_df)
        df_port = pd.DataFrame({'port': individual_port_val_df.sum(axis=1)})
        df_all = pd.concat([df_port, df_index], axis=1).dropna().rebase()

        # ‚úÖ ÏãúÍ∞ÅÌôîÎ•º ÏúÑÌïú Ï†ÄÏû•
        st.session_state["latest_history_chart"] = df_all

        return df_all.tail().to_markdown()

    except Exception as e:
        return f"‚ùó Î∞±ÌÖåÏä§Ìä∏ ÎèÑÏ§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"


@tool
def plot_history_chart() -> str:
    """
    Í∞ÄÏû• ÏµúÍ∑ºÏóê Ï°∞ÌöåÌïú Ï£ºÍ∞Ä Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú Ï∞®Ìä∏Î•º ÏãúÍ∞ÅÌôîÌï©ÎãàÎã§.
    
    - Îã®Ïùº Ï¢ÖÎ™© (get_yf_stock_history Ìò∏Ï∂ú Ïãú): Ï¢ÖÍ∞Ä Ï∞®Ìä∏
    - Î≥µÏàò Ï¢ÖÎ™© (get_yf_cumulative_returns_tool Ìò∏Ï∂ú Ïãú): ÎàÑÏ†Å ÏàòÏùµÎ•† Ï∞®Ìä∏
    """
    chart_data = st.session_state.get("latest_history_chart")
    cum_rtn_data = st.session_state.get("latest_cum_rtn_df")

    if chart_data is not None and not chart_data.empty:
        st.subheader("üìà Îã®Ïùº Ï¢ÖÎ™© Ï£ºÍ∞Ä ÌûàÏä§ÌÜ†Î¶¨ Ï∞®Ìä∏")
        st.line_chart(chart_data, use_container_width=True)
        return "‚úÖ Îã®Ïùº Ï¢ÖÎ™© Ï£ºÍ∞Ä Ï∞®Ìä∏ ÏãúÍ∞ÅÌôî ÏôÑÎ£å"

    elif cum_rtn_data is not None and not cum_rtn_data.empty:
        st.subheader("üìà Î≥µÏàò Ï¢ÖÎ™© ÎàÑÏ†Å ÏàòÏùµÎ•† ÎπÑÍµê Ï∞®Ìä∏")
        st.line_chart(cum_rtn_data, use_container_width=True)
        return "‚úÖ ÎàÑÏ†Å ÏàòÏùµÎ•† Ï∞®Ìä∏ ÏãúÍ∞ÅÌôî ÏôÑÎ£å"

    else:
        return "‚ùó ÏãúÍ∞ÅÌôîÌï† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§. Î®ºÏ†Ä get_yf_stock_history ÎòêÎäî get_yf_cumulative_returns_toolÏùÑ Ìò∏Ï∂úÌï¥Ï£ºÏÑ∏Ïöî."



@tool
def get_backtest_summary_tool(ticker_list: list[str], period: str = "5y") -> str:
    """
    Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ ÏÑ±Í≥ºÏßÄÌëú ÏöîÏïΩ (Total Return, CAGR, Sharpe, MDD, Volatility, 1M/3M/6M/YTD/1Y ÏàòÏùµÎ•†)
    """
    try:
        df_close = get_yf_close_prices(ticker_list, period)
        df_index = get_yf_close_prices(["SPY", "QQQ"], period)

        rebal_dates = get_rebal_dates(df_close, 'month')
        n_assets = len(df_close.columns)

        weight_df = pd.DataFrame(
            [[1 / n_assets] * n_assets] * len(rebal_dates),
            index=rebal_dates,
            columns=df_close.columns
        )

        portval_df = calculate_portvals(df_close, weight_df)
        port_series = portval_df.sum(axis=1)

        df_all = pd.concat([port_series.rename("port"), df_index], axis=1).dropna()
        rtn = get_returns_df(df_all, log=True)
        cum_rtn = get_cum_returns_df(rtn, log=True)

        today = df_all.index[-1]
        years = (today - df_all.index[0]).days / 365

        def get_cagr(series):
            return (series.iloc[-1] / series.iloc[0]) ** (1 / years) - 1

        def get_sharpe(series, rf=0.025):
            excess = series.mean() * 252 - rf
            return excess / (series.std() * np.sqrt(252))

        def get_mdd(series):
            cummax = series.cummax()
            dd = series / cummax - 1
            return dd.min()

        def get_weekly_vol(series):
            weekly_rtn = series.resample("W").last().pct_change().dropna()
            return weekly_rtn.std() * np.sqrt(52)

        def get_return_since(series, from_date):
            from_date = pd.to_datetime(from_date)
            if from_date not in series.index:
                from_date = series.index[series.index.get_indexer([from_date], method='nearest')[0]]
            return (series.loc[series.index[-1]] / series.loc[from_date]) - 1

        summary_data = []
        for col in df_all.columns:
            s_cum = cum_rtn[col]
            s_log = rtn[col]
            s_price = df_all[col]

            total_rtn = s_cum.iloc[-1] - 1
            cagr = get_cagr(s_cum)
            sharpe = get_sharpe(s_log)
            mdd = get_mdd(s_cum)
            vol = get_weekly_vol(s_price)

            one_month = today - pd.Timedelta(days=30)
            three_month = today - pd.Timedelta(days=90)
            six_month = today - pd.Timedelta(days=180)
            one_year = today - pd.Timedelta(days=365)
            ytd_start = pd.Timestamp(year=today.year, month=1, day=1)

            r_1m = get_return_since(s_cum, one_month)
            r_3m = get_return_since(s_cum, three_month)
            r_6m = get_return_since(s_cum, six_month)
            r_1y = get_return_since(s_cum, one_year)
            r_ytd = get_return_since(s_cum, ytd_start)

            summary_data.append([
                f"{total_rtn * 100:.2f}%",
                f"{cagr * 100:.2f}%",
                f"{sharpe:.2f}",
                f"{mdd * 100:.2f}%",
                f"{vol * 100:.2f}%",
                f"{r_1m * 100:.2f}%",
                f"{r_3m * 100:.2f}%",
                f"{r_6m * 100:.2f}%",
                f"{r_ytd * 100:.2f}%",
                f"{r_1y * 100:.2f}%",
            ])

        summary_df = pd.DataFrame(
            summary_data,
            columns=["Total Return", "CAGR", "Sharpe", "Max Drawdown", "Volatility (Weekly)",
                     "1M", "3M", "6M", "YTD", "1Y"],
            index=df_all.columns
        )

        return "üìä Î∞±ÌÖåÏä§Ìä∏ ÏÑ±Í≥º ÏöîÏïΩ:\n\n" + summary_df.to_markdown()

    except Exception as e:
        return f"‚ùó ÏÑ±Í≥º ÏöîÏïΩ Í≥ÑÏÇ∞ Ï§ë Ïò§Î•ò: {str(e)}"


# SSL Ïù∏Ï¶ùÏÑú Ïö∞Ìöå
@tool
def get_news_list(company_name: str, display=10) -> list:
    '''
    Naver Îâ¥Ïä§ APIÎ°ú Í∏∞ÏóÖ Í¥ÄÎ†® Îâ¥Ïä§ Í≤ÄÏÉâ ÌõÑ Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Î∞òÌôòÌï©ÎãàÎã§.
    '''
    client_id = os.getenv("NAVER_CLIENT_ID")  # ÌôòÍ≤Ω Î≥ÄÏàòÏóêÏÑú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ID Í∞ÄÏ†∏Ïò§Í∏∞
    client_secret = os.getenv("NAVER_CLIENT_SECRET")  # ÌôòÍ≤Ω Î≥ÄÏàòÏóêÏÑú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Secret Í∞ÄÏ†∏Ïò§Í∏∞
    
    # ÏøºÎ¶¨ URL Ïù∏ÏΩîÎî©
    query = urllib.parse.quote(company_name)
    url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display={display}&sort=date"
    
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }

    response = requests.get(url, headers=headers)
    
    if response.status_code != 200:
        print(f"Error: {response.status_code} - {response.text}")
        return []
    
    print("API Ìò∏Ï∂ú ÏÑ±Í≥µ!")
    items = response.json().get("items", [])
    
    if not items:
        print(f"‚ùó '{company_name}' Í¥ÄÎ†® Îâ¥Ïä§Í∞Ä ÏóÜÏäµÎãàÎã§.")
        return []

    # Îâ¥Ïä§ Í∏∞ÏÇ¨ ÎÇ¥Ïö© Ï†ïÏ†ú Ìï®Ïàò
    def clean(text):
        return html.unescape(BeautifulSoup(text, "html.parser").get_text())

    news_items = []
    
    for item in items:
        title = clean(item.get("title", "Ï†úÎ™© ÏóÜÏùå"))
        desc = clean(item.get("description", ""))
        link = item.get("originallink") or item.get("link") or f"https://search.naver.com/search.naver?query={query}"
        pub_date = item.get("pubDate", "ÎÇ†Ïßú ÏóÜÏùå")  # Í∏∞ÏÇ¨ Î∞úÌñâ ÎÇ†Ïßú
        
        # Î∞úÌñâ ÎÇ†Ïßú ÌòïÏãù Î≥ÄÌôò
        try:
            pub_date = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %z').strftime('%Y-%m-%d %H:%M:%S')
        except Exception as e:
            pub_date = "ÎÇ†Ïßú Î≥ÄÌôò Ïò§Î•ò"  # ÎÇ†Ïßú ÌòïÏãù Ïò§Î•ò Ïãú Í∏∞Î≥∏ Î©îÏãúÏßÄ
            
        # Í≤∞Í≥ºÏóê Ï†úÎ™©, ÎßÅÌÅ¨, ÏÑ§Î™Ö, ÎÇ†Ïßú Ï∂îÍ∞Ä
        news_items.append({
            "Ï†úÎ™©": title,
            "ÎÇ¥Ïö©": desc,
            "ÎßÅÌÅ¨": link,
            "Î∞úÌñâ ÎÇ†Ïßú": pub_date  # ÎÇ†Ïßú ÌòïÏãù Ï∂îÍ∞Ä
        })
    
    return news_items




@tool
def get_naver_news_sentiment(company_name: str, display: int = 5) -> str:
    """
    NAVER Îâ¥Ïä§ÏóêÏÑú Í∏∞ÏóÖ Í¥ÄÎ†® ÏµúÏã† Îâ¥Ïä§ Í≤ÄÏÉâ ‚Üí GPTÎ°ú Í¥ÄÎ†®ÏÑ± ÌïÑÌÑ∞ÎßÅ + ÌïúÍ∏Ä ÏöîÏïΩ + Í∞êÏÑ± Î∂ÑÏÑù.
    Í≤∞Í≥ºÎäî Í∏∞ÏÇ¨ Ï†úÎ™©, ÎßÅÌÅ¨, ÏöîÏïΩ, Í∞êÏÑ±ÏúºÎ°ú Íµ¨ÏÑ±Îê©ÎãàÎã§.
    """
    try:
        # 1. NAVER Îâ¥Ïä§ API Ìò∏Ï∂ú
        client_id = os.getenv("NAVER_CLIENT_ID")
        client_secret = os.getenv("NAVER_CLIENT_SECRET")
        query = urllib.parse.quote(company_name)
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display={display}&sort=date"
        headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }

        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            return f"‚ùó NAVER Îâ¥Ïä§ API Ìò∏Ï∂ú Ïò§Î•ò: {response.status_code}"

        items = response.json().get("items", [])
        if not items:
            return f"‚ùó '{company_name}' Í¥ÄÎ†® Îâ¥Ïä§Í∞Ä ÏóÜÏäµÎãàÎã§."

        # 2. Í∏∞ÏÇ¨ Ï†ïÏ†ú
        def clean(text):
            return html.unescape(BeautifulSoup(text, "html.parser").get_text())

        articles = []
        for i, item in enumerate(items):
            title = clean(item.get("title", "Ï†úÎ™© ÏóÜÏùå"))
            desc = clean(item.get("description", ""))
            link = item.get("originallink") or item.get("link") or f"https://search.naver.com/search.naver?query={query}"
            articles.append(f"{i+1}. Ï†úÎ™©: {title}\nÎÇ¥Ïö©: {desc}\nÎßÅÌÅ¨: {link}")

        news_text = "\n\n".join(articles)

        # 3. GPT ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
        prompt = f"""
'{company_name}'Ïù¥ÎùºÎäî ÌÇ§ÏõåÎìúÎ°ú Í≤ÄÏÉâÎêú Îâ¥Ïä§ Î™©Î°ùÏûÖÎãàÎã§. ÌïòÏßÄÎßå Ïù¥ Ï§ë ÏùºÎ∂ÄÎäî Í¥ÄÎ†® ÏóÜÎäî Í∏∞ÏÇ¨Ïùº Ïàò ÏûàÏäµÎãàÎã§
(Ïòà: Ïï†ÌîåÎØºÌä∏, Ïï†ÌîåÏö∞Îìú Îì±). ÏïÑÎûò ÏûëÏóÖÏùÑ ÏàòÌñâÌïòÏÑ∏Ïöî:

1. Ïã§Ï†ú Í∏∞ÏóÖ '{company_name}'Í≥º Í¥ÄÎ†® ÏûàÎäî Í∏∞ÏÇ¨Îßå Í≥®ÎùºÏ£ºÏÑ∏Ïöî.
2. Í∞Å Í∏∞ÏÇ¨Î•º ÌïúÍµ≠Ïñ¥Î°ú ÏûêÏó∞Ïä§ÎüΩÍ≤å ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî (Î≥¥ÎèÑÎ¨∏ Ïä§ÌÉÄÏùºÎ°ú).
3. Í∞Å Í∏∞ÏÇ¨Î≥ÑÎ°ú 'Í∏çÏ†ï', 'Î∂ÄÏ†ï', 'Ï§ëÎ¶Ω' Ï§ë Í∞êÏÑ±ÏùÑ ÌåêÎã®Ìï¥Ï£ºÏÑ∏Ïöî.
4. Í∞Å Í∏∞ÏÇ¨Ïóê Ï†úÎ™©/ÎßÅÌÅ¨/ÏöîÏïΩ/Í∞êÏÑ±ÏùÑ Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî.

[Îâ¥Ïä§ Î™©Î°ù]
{news_text}

üí¨ Ï∂úÎ†• ÏòàÏãú:
### 1. AI Ìà¨Ïûê ÌôïÎåÄ  
ÎßÅÌÅ¨: https://example.com/apple-ai  
ÏöîÏïΩ: ...  
Í∞êÏÑ±: Í∏çÏ†ï
"""
        gpt_response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )

        return gpt_response.choices[0].message.content.strip()

    except Exception as e:
        return f"‚ùó Îâ¥Ïä§ ÏöîÏïΩ ÎòêÎäî Í∞êÏÑ± Î∂ÑÏÑù Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"
    
@tool
def get_google_trend_tool(keyword: str, geo: str = "world") -> str:
    """
    Google Í≤ÄÏÉâ Ìä∏Î†åÎìú Îç∞Ïù¥ÌÑ∞Î•º Í∏∞Î∞òÏúºÎ°ú Í≤ÄÏÉâÎüâ Î≥ÄÌôîÎ•º Î∂ÑÏÑùÌï©ÎãàÎã§.
    - keyword: Í≤ÄÏÉâÌï† ÌÇ§ÏõåÎìú ÎòêÎäî Í∏∞ÏóÖÎ™Ö
    - geo: Íµ≠Í∞Ä ÏΩîÎìú (Ïòà: 'KR', 'US', 'JP'; Í∏∞Î≥∏Í∞í 'world')
    """
    with no_ssl_verification():
        try:
            # 1. pytrends ÏÑ∏ÌåÖ
            pytrends = TrendReq(hl="en-US", tz=360)
            geo_code = "" if geo.lower() == "world" else geo.upper()

            # 2. Ìä∏Î†åÎìú ÏöîÏ≤≠
            pytrends.build_payload([keyword], cat=0, timeframe="today 3-m", geo=geo_code, gprop="")

            data = pytrends.interest_over_time()
            if data.empty:
                return f"‚ùó '{keyword}'Ïóê ÎåÄÌïú Í≤ÄÏÉâ Ìä∏Î†åÎìú Îç∞Ïù¥ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."

            trend_series = data[keyword]

            # 3. Streamlit Ï∞®Ìä∏ Ï∂úÎ†•
            st.subheader(f"üîç ÏµúÍ∑º 3Í∞úÏõî Google Í≤ÄÏÉâ Ìä∏Î†åÎìú: '{keyword}' ({geo_code or 'Global'})")
            st.line_chart(trend_series)

            # 4. ÏöîÏïΩ Î∂ÑÏÑù
            recent = trend_series.iloc[-1]
            peak = trend_series.max()
            avg = trend_series.mean()
            min_ = trend_series.min()

            result = (
                f"‚úÖ '{keyword}'Ïóê ÎåÄÌïú ÏµúÍ∑º 3Í∞úÏõîÍ∞Ñ Í≤ÄÏÉâ Ìä∏Î†åÎìú Î∂ÑÏÑù:\n"
                f"- üîº ÏµúÍ≥†Ïπò: {peak:.0f}\n"
                f"- üîΩ ÏµúÏ†ÄÏπò: {min_:.0f}\n"
                f"- üìä ÌèâÍ∑†: {avg:.1f}\n"
                f"- üìÖ ÏµúÍ∑º Í≤ÄÏÉâÎüâ (ÎßàÏßÄÎßâÏùº Í∏∞Ï§Ä): {recent:.0f}"
            )
            return result

        except Exception as e:
            return f"‚ùó Google Ìä∏Î†åÎìú Î∂ÑÏÑù Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"

@tool
def compare_google_trend_tool(keywords: list[str], geo: str = "world") -> str:
    """
    Ïó¨Îü¨ ÌÇ§ÏõåÎìúÏóê ÎåÄÌïú Google Í≤ÄÏÉâ Ìä∏Î†åÎìúÎ•º ÎπÑÍµê Î∂ÑÏÑùÌï©ÎãàÎã§.
    - keywords: Í≤ÄÏÉâÏñ¥ Î¶¨Ïä§Ìä∏ (Ïòà: ['Apple', 'Microsoft'])
    - geo: Íµ≠Í∞Ä ÏΩîÎìú (Í∏∞Î≥∏: 'world' ‚Üí Ï†ÑÏÑ∏Í≥Ñ, Ïòà: 'KR', 'US')
    """
    with no_ssl_verification():
        try:
            if not keywords or len(keywords) < 2:
                return "‚ùó 2Í∞ú Ïù¥ÏÉÅÏùò ÌÇ§ÏõåÎìúÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî."

            pytrends = TrendReq(hl="en-US", tz=360)
            geo_code = "" if geo.lower() == "world" else geo.upper()

            pytrends.build_payload(keywords, cat=0, timeframe="today 3-m", geo=geo_code, gprop="")

            data = pytrends.interest_over_time()
            if data.empty:
                return f"‚ùó '{', '.join(keywords)}'Ïóê ÎåÄÌïú Í≤ÄÏÉâ Ìä∏Î†åÎìú Îç∞Ïù¥ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."

            data = data.drop(columns=["isPartial"], errors="ignore")

            # ‚úÖ Ï∞®Ìä∏ Ï∂úÎ†•
            st.subheader(f"üîç ÏµúÍ∑º 3Í∞úÏõî Í≤ÄÏÉâ Ìä∏Î†åÎìú ÎπÑÍµê: {', '.join(keywords)}")
            st.line_chart(data)

            # ‚úÖ ÏöîÏïΩ Î©îÏãúÏßÄ ÏÉùÏÑ±
            summaries = []
            for kw in keywords:
                recent = data[kw].iloc[-1]
                peak = data[kw].max()
                avg = data[kw].mean()
                summaries.append(
                    f"- **{kw}** ‚Üí üîº ÏµúÍ≥†: {peak:.0f}, üìä ÌèâÍ∑†: {avg:.1f}, üìÖ ÏµúÍ∑º: {recent:.0f}"
                )

            return "‚úÖ Google Í≤ÄÏÉâ Ìä∏Î†åÎìú ÎπÑÍµê Í≤∞Í≥º:\n" + "\n".join(summaries)

        except Exception as e:
            return f"‚ùó Google Ìä∏Î†åÎìú ÎπÑÍµê Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}"




# ÎèÑÍµ¨ Î∞îÏù∏Îî©
# ----- ÎèÑÍµ¨ Î∞îÏù∏Îî© -----
tools = [
    get_current_time,
    get_yf_stock_info,
    get_yf_stock_history,
    get_yf_cumulative_returns_tool,
    get_yf_stock_recommendations,
    plot_history_chart,  # ‚úÖ Ï∂îÍ∞Ä
    get_backtest_tool,  # ‚úÖ Ï∂îÍ∞Ä
    get_backtest_summary_tool,
    get_google_trend_tool, 
    compare_google_trend_tool,  # ‚úÖ Ï∂îÍ∞Ä
    get_news_list,
    get_naver_news_sentiment,
]

# nameÏùÑ ÌÇ§Î°ú ÌïòÎäî dict ÏÉùÏÑ±
tool_dict = {tool.name: tool for tool in tools}

llm_with_tools = llm.bind_tools(tools)


# ÏÇ¨Ïö©ÏûêÏùò Î©îÏãúÏßÄ Ï≤òÎ¶¨ÌïòÍ∏∞ ÏúÑÌïú Ìï®Ïàò
def get_ai_response(messages):
    response = llm_with_tools.stream(messages)  # ‚ë† LLM + tool ÏÇ¨Ïö©

    gathered = None  # ‚ë° Ï†ÑÏ≤¥ Î©îÏãúÏßÄÎ•º ÎàÑÏ†ÅÌï† Î≥ÄÏàò

    for chunk in response:
        yield chunk

        if gathered is None:
            gathered = chunk
        else:
            gathered += chunk

    # ‚ë¢ tool_callsÍ∞Ä ÏûàÎã§Î©¥, Ìï¥Îãπ tool Ìò∏Ï∂ú ÌõÑ ToolMessageÎ°ú ÏùëÎãµ
    if gathered.tool_calls:
        st.session_state.messages.append(gathered)

        for tool_call in gathered.tool_calls:
            try:
                selected_tool = tool_dict[tool_call["name"]]
                tool_output = selected_tool.invoke(tool_call)

                tool_msg = ToolMessage(
                    tool_call_id=tool_call["id"],
                    content=str(tool_output)
                )
                st.session_state.messages.append(tool_msg)

            except Exception as e:
                st.session_state.messages.append(ToolMessage(
                    tool_call_id=tool_call["id"],
                    content=f"‚ùå ÎèÑÍµ¨ Ìò∏Ï∂ú Ïã§Ìå®: {str(e)}"
                ))

        # ‚ë£ tool Ïã§Ìñâ ÌõÑ, Ïû¨Í∑ÄÏ†ÅÏúºÎ°ú LLM Ìò∏Ï∂ú
        for chunk in get_ai_response(st.session_state.messages):
            yield chunk


# Streamlit Ïï±
st.title("üí¨ GPT-4o Stock Chain Bot")

with st.expander("üìå ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäî Í∏∞Îä• ÏöîÏïΩ Î≥¥Í∏∞"):
    st.markdown("""
### üéØ ÏßÄÏõê Í∏∞Îä• Î∞è ÏòàÏãú Î™ÖÎ†πÏñ¥

| Í∏∞Îä• Ïù¥Î¶Ñ | ÏÑ§Î™Ö | üîç ÏòàÏãú Î™ÖÎ†πÏñ¥ |
|-----------|------|----------------|
| `get_current_time` | ÏßÄÏ†ïÌïú ÌÉÄÏûÑÏ°¥Ïùò ÌòÑÏû¨ ÏãúÍ∞ÑÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§. | `"ÏÑúÏö∏Ïùò ÌòÑÏû¨ ÏãúÍ∞ÑÏùÑ ÏïåÎ†§Ï§ò"` |
| `get_yf_stock_info` | ÏûÖÎ†•Ìïú Ï¢ÖÎ™© Ìã∞Ïª§Ïùò Yahoo Finance Ï†ïÎ≥¥Î•º Î∞òÌôòÌï©ÎãàÎã§. | `"AAPL Ï¢ÖÎ™© Ï†ïÎ≥¥ ÏïåÎ†§Ï§ò"` |
| `get_yf_stock_history` | Ï¢ÖÎ™©Ïùò Ï£ºÍ∞Ä Ïù¥Î†•ÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§. | `"TSLAÏùò ÏµúÍ∑º Ï£ºÍ∞Ä ÌùêÎ¶Ñ Î≥¥Ïó¨Ï§ò"` |
| `get_yf_cumulative_returns_tool` | Ïó¨Îü¨ Ï¢ÖÎ™©Ïùò ÎàÑÏ†Å ÏàòÏùµÎ•†ÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§. | `"TSLA, PLTRÏùò 1ÎÖÑ ÎàÑÏ†ÅÏàòÏùµÎ•† ÌùêÎ¶Ñ Î≥¥Ïó¨Ï§ò"` |
| `get_yf_stock_recommendations` | Ïï†ÎÑêÎ¶¨Ïä§Ìä∏Îì§Ïùò Ï¢ÖÎ™© Ï∂îÏ≤ú Îç∞Ïù¥ÌÑ∞Î•º Î≥¥Ïó¨Ï§çÎãàÎã§. | `"NVDAÏóê ÎåÄÌïú Î¶¨ÏÑúÏπò Ï∂îÏ≤ú Î≥¥Ïó¨Ï§ò"` |
| `plot_history_chart` | Ï£ºÍ∞Ä Ïù¥Î†•ÏùÑ ÏãúÍ∞ÅÌôîÌï©ÎãàÎã§. (Ïù¥Ï†Ñ Ï°∞Ìöå ÌïÑÏöî) | `"Ï∞®Ìä∏Î°ú Î≥¥Ïó¨Ï§ò"` |
| `get_backtest_tool` | Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ ÎàÑÏ†Å ÏàòÏùµÎ•† Î∞±ÌÖåÏä§Ìä∏ Ïã§Ìñâ | `"AAPLÍ≥º MSFTÎ°ú 5ÎÖÑ Î∞±ÌÖåÏä§Ìä∏ Ìï¥Ï§ò"` |
| `get_backtest_summary_tool` | Ìè¨Ìä∏ ÏàòÏùµÎ•†, Î≥ÄÎèôÏÑ±, ÏÉ§ÌîÑÏßÄÏàò Îì± ÏöîÏïΩ | `"ÌÖåÏä¨ÎùºÏôÄ ÏóîÎπÑÎîîÏïÑÎ°ú Ìè¨Ìä∏ ÏÑ±Í≥º ÏöîÏïΩÌï¥Ï§ò"` |
| `get_naver_news_sentiment` | NAVER Îâ¥Ïä§ÏóêÏÑú Í¥ÄÎ†® Îâ¥Ïä§ Í≤ÄÏÉâ + Í∞êÏÑ± Î∂ÑÏÑù | `"ÏÇºÏÑ±Ï†ÑÏûê Í¥ÄÎ†® Îâ¥Ïä§ Í∞êÏÑ± Î∂ÑÏÑùÌï¥Ï§ò"` |
| `get_company_news_with_sentiment` | Naver Îâ¥Ïä§ÏóêÏÑú Í∏∞ÏÇ¨ + Í∞êÏÑ± Ï†êÏàòÎ°ú ÏöîÏïΩ | `"LGÏóêÎÑàÏßÄÏÜîÎ£®ÏÖò Îâ¥Ïä§ Í∞êÏÑ± ÏöîÏïΩ"` |
| `get_google_trend_tool` | Google Í≤ÄÏÉâ Ìä∏Î†åÎìú Î∂ÑÏÑù (ÏµúÍ∑º 3Í∞úÏõî) | `"Ïï†Ìîå Í≤ÄÏÉâ Ìä∏Î†åÎìú Î≥¥Ïó¨Ï§ò"` |
| `compare_google_trend_tool` | Ïó¨Îü¨ ÌÇ§ÏõåÎìú Í≤ÄÏÉâÎüâ ÎπÑÍµê | `"ÏÇºÏÑ±Ï†ÑÏûêÏôÄ Ïï†Ìîå Í≤ÄÏÉâÎüâ ÎπÑÍµê"` |
| `reddit_stock_summary_tool` | RedditÏóêÏÑú Ïù∏Í∏∞ Í≤åÏãúÍ∏Ä ÏàòÏßë | `"wallstreetbetsÏóêÏÑú ÏµúÍ∑º Ïù∏Í∏∞ Í∏Ä Í∞ÄÏ†∏ÏôÄÏ§ò"` |
    """)


# Ïä§Ìä∏Î¶ºÎ¶ø session_stateÏóê Î©îÏãúÏßÄ Ï†ÄÏû•
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        SystemMessage("ÎÑàÎäî Ìà¨ÏûêÏûêÎ•º ÎèïÍ∏∞ ÏúÑÌï¥ ÏµúÏÑ†ÏùÑ Îã§ÌïòÎäî Ïù∏Í≥µÏßÄÎä• Î¥áÏù¥Îã§."),  
        AIMessage("How can I help you?")
    ]

# Ïä§Ìä∏Î¶ºÎ¶ø ÌôîÎ©¥Ïóê Î©îÏãúÏßÄ Ï∂úÎ†•
for msg in st.session_state.messages:
    if msg.content:
        if isinstance(msg, SystemMessage):
            st.chat_message("system").write(msg.content)
        elif isinstance(msg, AIMessage):
            st.chat_message("assistant").write(msg.content)
        elif isinstance(msg, HumanMessage):
            st.chat_message("user").write(msg.content)
        elif isinstance(msg, ToolMessage):
            st.chat_message("tool").write(msg.content)


# ÏÇ¨Ïö©Ïûê ÏûÖÎ†• Ï≤òÎ¶¨
if prompt := st.chat_input():
    st.chat_message("user").write(prompt) # ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ Ï∂úÎ†•
    st.session_state.messages.append(HumanMessage(prompt)) # ÏÇ¨Ïö©Ïûê Î©îÏãúÏßÄ Ï†ÄÏû•

    response = get_ai_response(st.session_state["messages"])
    
    result = st.chat_message("assistant").write_stream(response) # AI Î©îÏãúÏßÄ Ï∂úÎ†•
    st.session_state["messages"].append(AIMessage(result)) # AI Î©îÏãúÏßÄ Ï†ÄÏû•    
    
    
with st.expander("üß† Reddit Ï¢ÖÎ™© ÏöîÏïΩ ÏßÅÏ†ë Ïã§Ìñâ"):
    if st.button("Reddit Ï¢ÖÎ™© Î∂ÑÏÑù Ïã§Ìñâ"):
        with st.spinner("Reddit Î∂ÑÏÑù Ï§ë..."):
            summary = run_reddit_summary(limit=30, subreddit="wallstreetbets")
            st.markdown(summary, unsafe_allow_html=True)